{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:09.069864Z",
     "start_time": "2022-02-02T16:45:09.063239Z"
    }
   },
   "outputs": [],
   "source": [
    "GROUP_NAME = \"RTDs\" # just a title for use in charts\n",
    "BRAND_NAME = \"Shine+\"\n",
    "CSV_EXPORT_NAME = \"Shine+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:09.245478Z",
     "start_time": "2022-02-02T16:45:09.237432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://www.shinedrink.com/': 'Shine+',\n",
       " 'http://rowdyenergy.com/': 'Rowdy Energy',\n",
       " 'http://drinkolipop.com/': 'Olipop',\n",
       " 'https://drinkpoppi.com/': 'Poppi',\n",
       " 'https://www.drinkaha.com/': 'Aha'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_name_di = {}\n",
    "\n",
    "brand_name_di['https://www.shinedrink.com/'] = 'Shine+'\n",
    "brand_name_di['http://rowdyenergy.com/'] = 'Rowdy Energy'\n",
    "brand_name_di['http://drinkolipop.com/'] = 'Olipop'\n",
    "brand_name_di['https://drinkpoppi.com/'] = 'Poppi'\n",
    "brand_name_di['https://www.drinkaha.com/'] = 'Aha'\n",
    "\n",
    "brand_name_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:09.408890Z",
     "start_time": "2022-02-02T16:45:09.405286Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def stand_url(url):\n",
    "    # this function searches for content after 'www.' and before next '/'\n",
    "    # If 'www.' doesn't exist, then it will search for '//' and return the content after '//' and before next '/'\n",
    "    # if both 'www.' and '//' don't exist,  then it will return original url before next '/'\n",
    "    if 'www.' in url:\n",
    "        pattern = re.compile(r'(?<=www.)(?:(?!/).)*')\n",
    "        result = pattern.search(url)\n",
    "        return result.group()\n",
    "    elif '//' in url:\n",
    "        pattern = re.compile(r'(?<=//)(?:(?!/).)*')\n",
    "        result = pattern.search(url)\n",
    "        return result.group()\n",
    "    else:\n",
    "        pattern2 = re.compile(r'(?:(?!/).)*')\n",
    "        result2 = pattern2.search(url) \n",
    "        return result2.group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:10.054740Z",
     "start_time": "2022-02-02T16:45:10.051021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shinedrink.com': 'Shine+',\n",
       " 'rowdyenergy.com': 'Rowdy Energy',\n",
       " 'drinkolipop.com': 'Olipop',\n",
       " 'drinkpoppi.com': 'Poppi',\n",
       " 'drinkaha.com': 'Aha'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize URLs\n",
    "brand_name_di = {stand_url(key): value for key, value in brand_name_di.items()}\n",
    "brand_name_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:10.236422Z",
     "start_time": "2022-02-02T16:45:10.234550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do you want to print all dataframes for QA?\n",
    "# WARNING: will make your script run for a longer time\n",
    "\n",
    "# 'Y' for Yes\n",
    "# 'N' or empty for No\n",
    "\n",
    "print_all_dataframes = 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:15.129165Z",
     "start_time": "2022-02-02T16:45:15.126968Z"
    }
   },
   "outputs": [],
   "source": [
    "# typically 100 - judgement call to increase/decrease\n",
    "# any lower than 80 might avoid \n",
    "MIN_REV_COUNT = 80 # Unique reviewers required for analysis (closedly related to but not quite reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T18:43:42.776909Z",
     "start_time": "2021-01-04T18:43:42.771376Z"
    }
   },
   "source": [
    "add/subtract kpcs from this list https://docs.google.com/spreadsheets/d/1jjexRCEuARAdghPv3FCoyZtJ6CuWBUG3vTofyGxQf3A/edit#gid=0 \n",
    "\n",
    "This list was generated based on topic modeling in early 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:15.480565Z",
     "start_time": "2022-02-02T16:45:15.478014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: review above google sheets KPC list (topics per category) to determine KPC words to add/exclude\n",
    "# KPC google sheet created Feb 2021 \n",
    "\n",
    "# Add if relevant for brand\n",
    "ADD_KPC_LI = [\n",
    "    'Flavor',\n",
    "    'Value for Money',\n",
    "    'Efficacy',\n",
    "    'Natural',\n",
    "    'Nutritionals'\n",
    "] # \n",
    "\n",
    "# Exclude if not relevant for brand\n",
    "EXCLUDE_KPC_LI = [\n",
    "#                 'Easy to use',\n",
    "#                  'Packaging'\n",
    "]\n",
    "\n",
    "# Determine closest category in Google Sheet to run for brand\n",
    "# Can also pull from diff categories, if the seedwords make sense for brand\n",
    "PRIMARY_CATEGORY_LI = [] # could be custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:15.664329Z",
     "start_time": "2022-02-02T16:45:15.662385Z"
    }
   },
   "outputs": [],
   "source": [
    "GROUP_NAME = \"RTDs\" # just a title for use in charts\n",
    "\n",
    "# additional words to filter before combining into ngrams\n",
    "CUSTOM_STOPWORDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:17.138177Z",
     "start_time": "2022-02-02T16:45:17.136000Z"
    }
   },
   "outputs": [],
   "source": [
    "# SENT - sentences\n",
    "# regex to break out sentences\n",
    "CUSTOM_SENT_SPLIT = \"[..|...|....|;|-]\"\n",
    "\n",
    "# min length - reviews with substance\n",
    "MIN_SENT_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:17.334788Z",
     "start_time": "2022-02-02T16:45:17.332986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#csv_input =  input(\"Enter all CSV locations EXACTLY as displayed in your repo, separated by a comma and space. \\nIf not adding CSVs, leave blank \\n \\n\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:17.513751Z",
     "start_time": "2022-02-02T16:45:17.511370Z"
    }
   },
   "outputs": [],
   "source": [
    "# if a brand sells more than just one type of product, should focus on the main product\n",
    "# format: dict of string (comma separated lower case words) in order matching that in NORMALIZED_URL_LI\n",
    "# if there is a string for one brand, must enter empty strings for all others to keep matching order.\n",
    "# in the future this can be replaced potentially by product ER (category or attributes)\n",
    "\n",
    "# multiple words for one brand 'farm to flock, layena'\n",
    "# make sure to pay attention to ordering (match brand_name_di order)\n",
    "\n",
    "PRODUCT_NAME_CONTAINS_LI = ['',\n",
    "                            '',\n",
    "                            '',\n",
    "                            '',\n",
    "                            ''\n",
    "] # the csv list is an OR operation, it includes product with ANY of the listed words\n",
    "\n",
    "PRODUCT_NAME_NOT_CONTAINS_LI = [\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    ''\n",
    "] # the csv list is an AND operation, it excludes product with ANY of the listed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:19.210752Z",
     "start_time": "2022-02-02T16:45:19.208694Z"
    }
   },
   "outputs": [],
   "source": [
    "TOP_P = 20 # top words per aspect to potentially add per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:19.383680Z",
     "start_time": "2022-02-02T16:45:19.381067Z"
    }
   },
   "outputs": [],
   "source": [
    "# cols to query in main reviews dataset\n",
    "REV_COLS = [\n",
    "    'source_name',\n",
    "    'normalized_url',\n",
    "    'review_source_id',\n",
    "    'reviewer_source_id',\n",
    "    'brand_name',\n",
    "    'product_name',\n",
    "    'product_source_id',\n",
    "    'review_date',\n",
    "    'review_rating',\n",
    "    'review_content'\n",
    "]\n",
    "\n",
    "# cols to query in later cell\n",
    "GROUP_COLS = [\n",
    "    'normalized_url',\n",
    "    'review_source_id',\n",
    "    'review_content',\n",
    "#   'source_name', # actually some of them are duplicates across sources\n",
    "    'top_asp',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:19.551885Z",
     "start_time": "2022-02-02T16:45:19.549637Z"
    }
   },
   "outputs": [],
   "source": [
    "NORMALIZED_URL_LI = list(brand_name_di.keys())\n",
    "BRAND_NAME_LI = list(brand_name_di.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:20.388273Z",
     "start_time": "2022-02-02T16:45:20.385147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shinedrink.com',\n",
       " 'rowdyenergy.com',\n",
       " 'drinkolipop.com',\n",
       " 'drinkpoppi.com',\n",
       " 'drinkaha.com']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMALIZED_URL_LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:21.965385Z",
     "start_time": "2022-02-02T16:45:21.962165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shine+', 'Rowdy Energy', 'Olipop', 'Poppi', 'Aha']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BRAND_NAME_LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:22.373042Z",
     "start_time": "2022-02-02T16:45:22.370087Z"
    }
   },
   "outputs": [],
   "source": [
    "product_name_include_di = dict(zip(NORMALIZED_URL_LI, [w.split(\", \") if w else [] for w in PRODUCT_NAME_CONTAINS_LI ]))\n",
    "product_name_exclude_di = dict(zip(NORMALIZED_URL_LI, [w.split(\", \") if w else [] for w in PRODUCT_NAME_NOT_CONTAINS_LI]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:22.632740Z",
     "start_time": "2022-02-02T16:45:22.629712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shinedrink.com': [],\n",
       " 'rowdyenergy.com': [],\n",
       " 'drinkolipop.com': [],\n",
       " 'drinkpoppi.com': [],\n",
       " 'drinkaha.com': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check outputs below to make sure product inclusion/exclusions is assigned as intended\n",
    "product_name_include_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:23.151154Z",
     "start_time": "2022-02-02T16:45:23.147908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shinedrink.com': [],\n",
       " 'rowdyenergy.com': [],\n",
       " 'drinkolipop.com': [],\n",
       " 'drinkpoppi.com': [],\n",
       " 'drinkaha.com': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check outputs below to make sure product inclusion/exclusions is assigned as intended\n",
    "product_name_exclude_di "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumes Spark 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some array function only in spark >2.3. I use docker image with statsmodels and plotly and vaderSentiment based on spark 2.4.5\n",
    "Please deloy cluster with `--docker-tag cici-nlp-sentiment` or other spark 2.4 docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:45:56.758056Z",
     "start_time": "2022-02-02T16:45:56.754121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-101-207.us-west-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://terry.cu:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://terry.cu:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:49:22.828344Z",
     "start_time": "2022-02-02T16:48:45.979600Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install watermark\n",
    "!pip3 install --upgrade --no-cache-dir --extra-index-url http://pypi.cu/root/circleup/+simple/ --trusted-host pypi.cu cu-helio-insights==0.0.12\n",
    "!pip install -Iv numpy==1.19.2\n",
    "!pip install ipynb\n",
    "!pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:49:23.236179Z",
     "start_time": "2022-02-02T16:49:22.829942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "py4j     0.10.7\n",
      "re       2.2.1\n",
      "platform 1.0.8\n",
      "CPython 3.6.8\n",
      "IPython 7.16.1\n",
      "\n",
      "compiler   : GCC 6.3.0 20170516\n",
      "system     : Linux\n",
      "release    : 4.9.43-17.39.amzn1.x86_64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m --iversions -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:54:03.511439Z",
     "start_time": "2022-02-02T16:54:03.320122Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/usr/local/lib/python3.6/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-46223072b995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#from pyspark.sql.functions import arrays_zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"numpy in {}. One method of fixing this is to repeatedly uninstall \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"numpy until none is found, then reinstall this version.\")\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerictypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/usr/local/lib/python3.6/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "#from pyspark.sql.functions import arrays_zip\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, HashingTF\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from itertools import chain\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "import insights\n",
    "from insights.investor_tools.widgets.style import (\n",
    "    Font,\n",
    "    CU_PLOTLY_COLOR_SEQUENCE,\n",
    "    CU_PLOTLY_COLORSCALE,\n",
    "    Color\n",
    ")\n",
    "from spark_tools import T, F, c, read_google_sheet\n",
    "import apollo\n",
    "from apollo import OverrideConfiguration\n",
    "from apollo import dataset\n",
    "import apollo_artifacts\n",
    "from apollo_artifacts import datasets\n",
    "from transform.attributes.utils import tf_idf_attributes\n",
    "\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:47:48.306231Z",
     "start_time": "2022-02-02T15:47:48.120180Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.292567Z",
     "start_time": "2022-02-02T15:38:40.736Z"
    }
   },
   "outputs": [],
   "source": [
    "## SUMMARY\n",
    "# Importing aspects of interest from KPC google sheet\n",
    "\n",
    "bootstrap_dict = {}\n",
    "\n",
    "from spark_tools import T, F, c, read_google_sheet\n",
    "GSHEET_LOC = \"https://docs.google.com/spreadsheets/d/1jjexRCEuARAdghPv3FCoyZtJ6CuWBUG3vTofyGxQf3A/\"\n",
    "GSHEET_NAME = 'top_per_category'\n",
    "\n",
    "# primary category\n",
    "top_seedwords_sdf = read_google_sheet(GSHEET_LOC, sheet=GSHEET_NAME)\n",
    "\n",
    "# place for the analyst to manually add?\n",
    "ADD_GSHEET_NAME = 'kpc_seedword_map'\n",
    "# seedwords\n",
    "add_seedword_sdf = read_google_sheet(GSHEET_LOC, sheet=ADD_GSHEET_NAME)\n",
    "\n",
    "# finding the primary category and terms here\n",
    "aspect_sdf = top_seedwords_sdf.filter(F.col('primary_category').isin(PRIMARY_CATEGORY_LI)).select('kpc_name', 'seed_words')\n",
    "\n",
    "# add any of the manually specified criteria here\n",
    "add_aspect_sdf =  add_seedword_sdf.filter(F.col('kpc_name').isin(ADD_KPC_LI)).select('kpc_name', 'seed_words')\n",
    "\n",
    "base_aspect_dict = aspect_sdf.union(add_aspect_sdf).rdd.map(lambda x : (x[0], x[1].split(', '))).collectAsMap()\n",
    "\n",
    "# remove any we specified as not wanting to keep\n",
    "for asp in EXCLUDE_KPC_LI:\n",
    "    del base_aspect_dict[asp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stopwords and functions .ipynb from HAI General S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the logic for pulling and cleaning reviews is shared between TF-IDF and KPC scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.293429Z",
     "start_time": "2022-02-02T15:38:40.738Z"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.294624Z",
     "start_time": "2022-02-02T15:38:40.740Z"
    }
   },
   "outputs": [],
   "source": [
    "# cd to where the helpers and config files are located\n",
    "%cd /circleup-notebooks/s3-circleup-notebooks/HAI/General/Code/TF_IDF\n",
    "\n",
    "from ipynb.fs.full.functions_tfidf import *\n",
    "\n",
    "# cd - command changes back to previously defined directory (should be same as og_filepath)\n",
    "%cd /circleup-notebooks/s3-circleup-notebooks/SPan/KPC\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.295714Z",
     "start_time": "2022-02-02T15:38:40.743Z"
    }
   },
   "outputs": [],
   "source": [
    "# cd to where the helpers and config files are located\n",
    "%cd /circleup-notebooks/s3-circleup-notebooks/HAI/General/helpers\n",
    "\n",
    "from ipynb.fs.full.general_helpers import *\n",
    "\n",
    "# cd - command changes back to previously defined directory (should be same as og_filepath)\n",
    "%cd /circleup-notebooks/s3-circleup-notebooks/SPan/KPC\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.296783Z",
     "start_time": "2022-02-02T15:38:40.745Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # set seed for models for reproducibility\n",
    "\n",
    "stopwords_li = stopwords.words('english')\n",
    "punkts_li = list(string.punctuation)\n",
    "\n",
    "pd.options.display.max_rows = 300\n",
    "OverrideConfiguration(default_to_production=True).apply()\n",
    "\n",
    "spark.conf.set('spark.sql.execution.arrow.enabled', 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.297749Z",
     "start_time": "2022-02-02T15:38:40.747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Issues adding to external helpers file\n",
    "# Why? - when you use F.udf, this is a \"whole new instance\" so you wont be able to use any of the stuff you just imported\n",
    "# Why do we need @F.udf? Unsure\n",
    "# NLTK package has sent_tokenize_udf, why don't we use this?\n",
    "     # Specific use case for Spark data? vs. Python data\n",
    "     # Using python function on spark data\n",
    "@F.udf(T.ArrayType(T.StringType()))\n",
    "def sent_tokenize_udf(s):\n",
    "    return sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.298494Z",
     "start_time": "2022-02-02T15:38:40.749Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "test_timeline_data = (\n",
    "apollo\n",
    ".dataset('online_reviews__deduped_review__1_0')\n",
    ".read()\n",
    ".select('normalized_url', 'execution_date')\n",
    ".where(c.execution_date > date.today() - timedelta(days=7))\n",
    ")\n",
    "\n",
    "max_date = test_timeline_data.where(c.normalized_url.isNotNull()).agg({'execution_date':'max'}).collect()[0][0]\n",
    "\n",
    "reviews = (\n",
    "dataset('online_reviews__deduped_review__1_0')\n",
    ".read_segment_df({'execution_date': max_date})\n",
    ".select(REV_COLS)\n",
    ".persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.299236Z",
     "start_time": "2022-02-02T15:38:40.751Z"
    }
   },
   "outputs": [],
   "source": [
    "panda(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to make sure 'normalized_url' is coming through properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.300643Z",
     "start_time": "2022-02-02T15:38:40.753Z"
    }
   },
   "outputs": [],
   "source": [
    "panda(reviews)['normalized_url'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Tweaking for Potential ER Issues\n",
    "- If we have reviews for a brand, but the URL is incorrect, we can fix it below\n",
    "- Manually fix brand id --> normalized url mapping issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.301807Z",
     "start_time": "2022-02-02T15:38:40.755Z"
    }
   },
   "outputs": [],
   "source": [
    "ER_BRAND_URL_MAP = {'thepoppy.store': 'drinkpoppi.com'}\n",
    "# ER_BRAND_URL_MAP = {'dae':'daehair.com'} # manually add {brand_id : normalized_url}\n",
    "\n",
    "URL_BLACKLIST = []\n",
    "# Some urls sneak into the list somehow. Add them here to remove them from the charts\n",
    "\n",
    "for bad_url, url in ER_BRAND_URL_MAP.items():\n",
    "    reviews = reviews.withColumn(\n",
    "        \"normalized_url\",\n",
    "        F.when(\n",
    "            F.col(\"normalized_url\") == bad_url,\n",
    "            url\n",
    "        ).otherwise(F.col(\"normalized_url\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for Minimal Rev Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.302988Z",
     "start_time": "2022-02-02T15:38:40.757Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews.filter(F.col('normalized_url').isin(NORMALIZED_URL_LI)).groupby('normalized_url').count().limit(20).toPandas().sort_values('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.304105Z",
     "start_time": "2022-02-02T15:38:40.760Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews.filter(F.col('normalized_url').isin(NORMALIZED_URL_LI)).groupby('normalized_url','source_name').count().limit(20).toPandas().sort_values('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.305185Z",
     "start_time": "2022-02-02T15:38:40.762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure URL meets min rev count\n",
    "\n",
    "count_revs = (\n",
    "    reviews\n",
    "    .dropna(subset=['normalized_url'])\n",
    "    .groupby('normalized_url')\n",
    "    .agg(\n",
    "        F.countDistinct(F.concat(F.col('source_name'), F.col('reviewer_source_id'))).alias('reviewer_count')\n",
    "    )\n",
    ")\n",
    "    \n",
    "count_revs_filtered = (\n",
    "    count_revs\n",
    "    .filter(F.col('reviewer_count') >= MIN_REV_COUNT)\n",
    "    .select('normalized_url')\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "filtered_revs = (\n",
    "    reviews\n",
    "    .join(count_revs_filtered, on='normalized_url')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check in Reviews with Enough Reviews of Reviews at All\n",
    "Check for ER misses here which occur frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.306356Z",
     "start_time": "2022-02-02T15:38:40.763Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_revs = filtered_revs.filter(F.col('normalized_url').isin(NORMALIZED_URL_LI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.307475Z",
     "start_time": "2022-02-02T15:38:40.765Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "(reviews\n",
    " .filter(F.col('normalized_url').isin(NORMALIZED_URL_LI))\n",
    " .cache()\n",
    " .groupby('normalized_url','product_name')\n",
    " .count()\n",
    " .sort('count', ascending=False)\n",
    " .limit(10).toPandas())\n",
    " \n",
    "#  .show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Filter for Product\n",
    "- include/exclude products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.308594Z",
     "start_time": "2022-02-02T15:38:40.767Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRODUCT_NAME_CONTAINS_LI:\n",
    "    brand_revs = filter_included_products(brand_revs, product_name_include_di, \"product_name\", \"normalized_url\")\n",
    "\n",
    "if PRODUCT_NAME_NOT_CONTAINS_LI:\n",
    "    brand_revs = filter_excluded_products(brand_revs, product_name_exclude_di, \"product_name\", \"normalized_url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Filter for Review Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cases: if there is a mapping issue or need to add a word filter on review content, use this function to exclude reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.309618Z",
     "start_time": "2022-02-02T15:38:40.769Z"
    }
   },
   "outputs": [],
   "source": [
    "exclude_word_li = [\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    ''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.310254Z",
     "start_time": "2022-02-02T15:38:40.771Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_revs = brand_revs.where(\n",
    "    ~brand_revs['review_content'].rlike(\"|\".join([\"(\" + pat + \")\" for pat in exclude_word_li]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check most common products to see if anything else to exclude; if yes go back up to product exclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.310976Z",
     "start_time": "2022-02-02T15:38:40.773Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "(brand_revs\n",
    ".filter(F.col('normalized_url').isin(NORMALIZED_URL_LI))\n",
    ".cache()\n",
    ".groupby('normalized_url','product_name')\n",
    ".count()\n",
    ".sort('count', ascending=False)\n",
    ".limit(30).toPandas())\n",
    "\n",
    " \n",
    "#  .show(50, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload CSV\n",
    "- DTC CSV input\n",
    "- Cases where we manually scrape DTC reviews, then input the CSV here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.311643Z",
     "start_time": "2022-02-02T15:38:40.780Z"
    }
   },
   "outputs": [],
   "source": [
    "external_revs_list = []\n",
    "\n",
    "for i in range(len(csv_input.split(', '))):\n",
    "    if csv_input != '':\n",
    "        external_revs_list.append(pd.read_csv((csv_input.split(', ')[i])))\n",
    "\n",
    "external_revs_list[0]['review_rating'] = external_revs_list[0]['review_rating'].str.replace('\\n','')\n",
    "external_revs_list[0]['review_rating'] = external_revs_list[0]['review_rating'].str.slice(6,7)\n",
    "external_revs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.312284Z",
     "start_time": "2022-02-02T15:38:40.782Z"
    }
   },
   "outputs": [],
   "source": [
    "# combining each of the external review lists\n",
    "from pyspark.sql.types import StructType\n",
    "mySchema = StructType(brand_revs.schema)\n",
    "if external_revs_list != []:\n",
    "    cleaned_external_review_list = []\n",
    "    for external_revs_csv in external_revs_list:  \n",
    "        external_revs_csv.rename(columns={'brand':'brand_name'},inplace=True)\n",
    "        external_revs_csv['review_rating']=pd.to_numeric(external_revs_csv['review_rating'], downcast='float')\n",
    "\n",
    "        external_revs_csv['source_name'] = 'DTC'\n",
    "        # Checking for static columns, to add blanksto data frame\n",
    "        # Then checking for incrementing columns, which we'll set as the index, assuming they are unique reviewers and reviews\n",
    "        static_columns = ['product_name']\n",
    "        # Add or remove columns that need to be populated, usually will be review_source_id, reviewer_source_id, or product_source_id if needed\n",
    "        incrementing_columns = ['review_source_id','reviewer_source_id', 'product_source_id']\n",
    "\n",
    "        for column_name in static_columns:\n",
    "            if column_name not in external_revs_csv:\n",
    "                external_revs_csv[column_name] = ''\n",
    "            else:\n",
    "                pass\n",
    "        for column_name in incrementing_columns:\n",
    "            external_revs_csv[column_name]=external_revs_csv.index\n",
    "\n",
    "        # selecting the same columns as internal reviews CSV\n",
    "        external_revs_csv = external_revs_csv[reviews.columns]\n",
    "        cleaned_external_review_list.append(external_revs_csv)\n",
    "\n",
    "    external_revs_csv = pd.concat(cleaned_external_review_list)\n",
    "    external_revs_csv = external_revs_csv[['normalized_url','source_name','review_source_id','reviewer_source_id','brand_name','product_name','product_source_id','review_date','review_rating','review_content']]\n",
    "    external_revs_sdf = spark.createDataFrame(external_revs_csv, schema = mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.314396Z",
     "start_time": "2022-02-02T15:38:40.784Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine the and clean review_content of dataframes, which should at this point have matching columns\n",
    "if external_revs_list != []:\n",
    "    brand_revs = brand_revs.union(external_revs_sdf)\n",
    "\n",
    "brand_revs = brand_revs.withColumn(\n",
    "    'review_content', remove_special_characters(\"review_content\")).alias('review_content')\n",
    "\n",
    "# removing hypertext from reviews\n",
    "brand_revs = brand_revs.withColumn(\n",
    "    'review_content', F.regexp_replace(\n",
    "        F.regexp_replace(\"review_content\", r\"(?i)<a\\s*[^>]*>\", \"\"), r\"(?i)<\\s*/\\s*a\\s*>\", \"\"\n",
    "    ).alias('review_content'))\n",
    "\n",
    "# removing special characters from reviews\n",
    "brand_revs = brand_revs.withColumn(\n",
    "    'review_content', remove_special_characters(\"review_content\")).alias('review_content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.315621Z",
     "start_time": "2022-02-02T15:38:40.785Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "import contractions\n",
    "# limiting because analysis can slow way down with too many reviews\n",
    "REVIEWS_PER_BRAND_TO_ANALYZE = 10_000\n",
    "\n",
    "#get most recent n reviews \n",
    "# strip contractions\n",
    "brand_revs_pdf = brand_revs.toPandas()\n",
    "brand_revs_pdf = (brand_revs_pdf\n",
    "                  .sort_values('review_date',ascending = False)\n",
    "                  .groupby('normalized_url')\n",
    "                  .head(REVIEWS_PER_BRAND_TO_ANALYZE))\n",
    "brand_revs_pdf.to_csv(BRAND_NAME+'_&comps_reviews.csv', index=False)\n",
    "#brand_revs_pdf['review_content'] = brand_revs_pdf['review_content'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "\n",
    "brand_revs = spark.createDataFrame(brand_revs_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.316679Z",
     "start_time": "2022-02-02T15:38:40.787Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_revs.groupby('normalized_url').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Iterative Bootstrapping\n",
    "This is the start of iterative process. If seedwords are sufficient, no need to run the below more than once. If adding to seedwords, once enough words are added, run one last time from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.317830Z",
     "start_time": "2022-02-02T15:38:40.789Z"
    }
   },
   "outputs": [],
   "source": [
    "aspect_dict = {key: value + bootstrap_dict.get(key, []) for key, value in base_aspect_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.319106Z",
     "start_time": "2022-02-02T15:38:40.791Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_rev_sents = tokenize_to_sentences(brand_revs, 'tokenized_sents', 'review_content', CUSTOM_SENT_SPLIT, MIN_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.320216Z",
     "start_time": "2022-02-02T15:38:40.793Z"
    }
   },
   "outputs": [],
   "source": [
    "token_brand_revs = (\n",
    "    brand_rev_sents\n",
    "    .withColumn('tokenized_text', lemmatize(tokenize(F.col('tokenized_sents'), F.lit(' '.join(CUSTOM_STOPWORDS)))))\n",
    "    .dropna(subset=['tokenized_text'])\n",
    ")\n",
    "\n",
    "unigram_sdf = tokenize_to_ngrams(token_brand_revs, 'uni', 'tokenized_word', 'tokenized_text')\n",
    "bigram_sdf = tokenize_to_ngrams(token_brand_revs, 'bi', 'bigram_word', 'tokenized_text')\n",
    "unibi_sdf = tokenize_to_ngrams(token_brand_revs, 'unibi', 'unibi_word', 'tokenized_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps tokenized words in reviews to the \"aspects\" or purchase criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.321371Z",
     "start_time": "2022-02-02T15:38:40.796Z"
    }
   },
   "outputs": [],
   "source": [
    "ASPECT_DICT = aspect_dict\n",
    "\n",
    "aspect_li = list(ASPECT_DICT.keys())\n",
    "\n",
    "inv_asp_map = {val: k for k, v in ASPECT_DICT.items() for val in v}\n",
    "\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*inv_asp_map.items())])\n",
    "\n",
    "full_asp_sdf = (\n",
    "    unigram_sdf\n",
    "#     unibi_sdf\n",
    "#     .withColumnRenamed('unibi_word', 'tokenized_word')\n",
    "    .withColumn('aspect', mapping_expr[F.col('tokenized_word')])\n",
    ")\n",
    "    \n",
    "asp_sdf = (\n",
    "    full_asp_sdf\n",
    "    .dropna(subset=['aspect'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.322431Z",
     "start_time": "2022-02-02T15:38:40.798Z"
    }
   },
   "outputs": [],
   "source": [
    "asp_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is gettting the \"top\" aspect per sentence per review, by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.323505Z",
     "start_time": "2022-02-02T15:38:40.802Z"
    }
   },
   "outputs": [],
   "source": [
    "KEEP_COLS = [\n",
    "    'normalized_url',\n",
    "    'review_source_id',\n",
    "    'review_content',\n",
    "    'source_name',\n",
    "    'tokenized_sents',\n",
    "    ]\n",
    "\n",
    "# window function not the fastest but preserves ties\n",
    "window = Window.partitionBy(KEEP_COLS).orderBy(F.col('count').desc())\n",
    "\n",
    "count_asp_sdf = (\n",
    "    asp_sdf\n",
    "    .groupby(KEEP_COLS+['aspect'])\n",
    "    .count()\n",
    "    .withColumn('rank', F.dense_rank().over(window))\n",
    ")\n",
    "\n",
    "top_asp_sdf = (\n",
    "    count_asp_sdf\n",
    "    .filter(F.col('rank') ==1)\n",
    "    .drop('rank', 'count')\n",
    "    .withColumnRenamed('aspect', 'top_asp')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.324628Z",
     "start_time": "2022-02-02T15:38:40.807Z"
    }
   },
   "outputs": [],
   "source": [
    "count_asp_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T17:24:05.215147Z",
     "start_time": "2020-09-03T17:24:05.136289Z"
    }
   },
   "source": [
    "The goal here is to see which terms appear disproportionately in aspects, which are NOT already part of the existing dictionary. Then, if necessary, can add those to the dictionary if it makes sense to include them.\n",
    "\n",
    "IMPORTANT: Skip steps 14 & 15 if you feel that you have sufficient seedwords from the base list.\n",
    "\n",
    "Chi squared value is derived from the formula using the following:\n",
    "* C1 is the number of times w occurs in sentences belonging to aspect Ai\n",
    "* C2 is the number of times w occurs in sentences not belonging to Ai\n",
    "* C3 is the number of sentences of aspect Ai that do not contain w\n",
    "* C4 is the number of sentences that neither belong to aspect Ai, nor contain word w\n",
    "* C is the total number of word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T16:52:35.574635Z",
     "start_time": "2022-02-02T16:52:35.571580Z"
    }
   },
   "outputs": [],
   "source": [
    "def chi_sq(C, C1, C2, C3, C4):\n",
    "    numer = C * (C1 * C4 - C2 * C3)**2\n",
    "    denom = (C1 + C3) * (C2 + C4) * (C1 + C2) * (C3 + C4)\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.326914Z",
     "start_time": "2022-02-02T15:38:40.813Z"
    }
   },
   "outputs": [],
   "source": [
    "assign_sdf = full_asp_sdf.select(KEEP_COLS + ['tokenized_word', 'aspect']).join(top_asp_sdf, on=KEEP_COLS, how='left')\n",
    "\n",
    "C = (\n",
    "    full_asp_sdf\n",
    "    .groupby('tokenized_word')\n",
    "    .count()\n",
    ")\n",
    "\n",
    "c_pdf = C.toPandas().set_index('tokenized_word').sort_index()\n",
    "c_arr = np.repeat(c_pdf.values, len(aspect_li), axis=1)\n",
    "\n",
    "# words x aspects, times w occurs in sentences of each aspect\n",
    "c1_sdf = (\n",
    "    assign_sdf\n",
    "    .drop('aspect')\n",
    "    .withColumnRenamed('top_asp', 'aspect')\n",
    "    .groupby('tokenized_word')\n",
    "    .pivot('aspect', aspect_li+[None])\n",
    "    .count()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "c3_prime_sdf = (\n",
    "    assign_sdf\n",
    "    .drop('aspect')\n",
    "    .withColumnRenamed('top_asp', 'aspect')\n",
    "    .groupby('tokenized_word')\n",
    "    .pivot('aspect', aspect_li+[None])\n",
    "    .agg(F.countDistinct('tokenized_sents'))\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "c1_pdf = c1_sdf.toPandas().set_index('tokenized_word')\n",
    "\n",
    "c1_pdf['total_count'] = c1_pdf[aspect_li+['null']].sum(axis=1)\n",
    "\n",
    "c2_pdf = np.tile(c1_pdf[['total_count']].values, len(aspect_li)) - c1_pdf[aspect_li]\n",
    "\n",
    "c3_prime_pdf = c3_prime_sdf.toPandas().set_index('tokenized_word')\n",
    "c3_prime_pdf['total_count'] = c3_prime_pdf[aspect_li+['null']].sum(axis=1)\n",
    "c3_pdf = np.tile(c3_prime_pdf[['total_count']].values, len(aspect_li)) - c3_prime_pdf[aspect_li]\n",
    "\n",
    "col_sum = c3_prime_pdf[aspect_li].sum(axis=0)\n",
    "c4_pdf = np.tile(col_sum, (len(c3_pdf),1)) - c3_pdf[aspect_li]\n",
    "\n",
    "chi_pdf = chi_sq(\n",
    "    c_arr,\n",
    "    c1_pdf[aspect_li].sort_index(),\n",
    "    c2_pdf[aspect_li].sort_index(),\n",
    "    c3_pdf[aspect_li].sort_index(),\n",
    "    c4_pdf[aspect_li].sort_index()\n",
    ")\n",
    "\n",
    "chi_melt_pdf = pd.melt(chi_pdf.reset_index(), id_vars='tokenized_word', value_vars=aspect_li)\n",
    "\n",
    "existing_word_pdf = pd.DataFrame.from_dict(inv_asp_map, orient='index').reset_index()\n",
    "existing_word_pdf.columns=['tokenized_word', 'variable']\n",
    "existing_word_pdf['existing'] = True\n",
    "\n",
    "chi_merge_pdf = chi_melt_pdf.merge(existing_word_pdf, how='left', on=['variable', 'tokenized_word'])\n",
    "chi_merge_pdf['existing'] = chi_merge_pdf['existing'].fillna(False)\n",
    "filtered_chi_pdf = chi_merge_pdf[chi_merge_pdf['existing']!=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.328028Z",
     "start_time": "2022-02-02T15:38:40.816Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_bootstrap_pdf = filtered_chi_pdf.sort_values(['variable', 'value'], ascending=False).groupby(['variable']).head(TOP_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary to include appropriate seedwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.329125Z",
     "start_time": "2022-02-02T15:38:40.818Z"
    }
   },
   "outputs": [],
   "source": [
    "bootstrap_dict = top_bootstrap_pdf.groupby('variable')['tokenized_word'].apply(list).to_frame()['tokenized_word'].to_dict()\n",
    "add_seed_dict = {aspect: [] for aspect in aspect_li}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.330249Z",
     "start_time": "2022-02-02T15:38:40.823Z"
    }
   },
   "outputs": [],
   "source": [
    "top_bootstrap_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.331348Z",
     "start_time": "2022-02-02T15:38:40.825Z"
    }
   },
   "outputs": [],
   "source": [
    "for aspect in bootstrap_dict.keys():\n",
    "    input_txt =  input(f\"Enter seedwords to include for KPC: {aspect} EXACTLY as displayed above, separated by a comma and space. \\nThis displays words that are not included in the base dictionary \\n \\n\").strip()\n",
    "\n",
    "    for i in range(len(input_txt.split(', '))):\n",
    "        add_seed_dict[aspect].append(input_txt.split(', ')[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.332455Z",
     "start_time": "2022-02-02T15:38:40.829Z"
    }
   },
   "outputs": [],
   "source": [
    "bootstrap_dict = add_seed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Iterative Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if sufficient words are added to augment the seedwords, go back to [here](#Start-Iterative-Bootstrapping) and rerun the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate on Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.333659Z",
     "start_time": "2022-02-02T15:38:40.833Z"
    }
   },
   "outputs": [],
   "source": [
    "# GROUP_COLS = [\n",
    "#     'normalized_url',\n",
    "#     'review_source_id',\n",
    "#     'review_content',\n",
    "# #   'source_name', # actually some of them are duplicates across sources\n",
    "#     'top_asp',\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.334764Z",
     "start_time": "2022-02-02T15:38:40.835Z"
    }
   },
   "outputs": [],
   "source": [
    "clustered_asp_sdf = (\n",
    "    top_asp_sdf\n",
    "    .groupby(GROUP_COLS)\n",
    "    .agg(\n",
    "        F.collect_list(F.col('tokenized_sents')).alias('sent_li')\n",
    "    )\n",
    "    .withColumn('combined_sents', F.array_join(F.col('sent_li'), ' '))\n",
    "    .drop('sent_li')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Intensity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T14:55:48.307454Z",
     "start_time": "2021-08-24T14:55:48.189867Z"
    }
   },
   "source": [
    "Need to decide if we want to keep this method of using sentiment analysis or if we want to just use ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.335846Z",
     "start_time": "2022-02-02T15:38:40.839Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = SIA()\n",
    "@F.udf(T.DoubleType())\n",
    "def get_sentiment_score(sentence):\n",
    "    pscores = sid.polarity_scores(sentence)\n",
    "    return pscores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.336502Z",
     "start_time": "2022-02-02T15:38:40.841Z"
    }
   },
   "outputs": [],
   "source": [
    "scored_asp_sdf = (\n",
    "    clustered_asp_sdf\n",
    "    .withColumn('polarity_score', get_sentiment_score(F.col('combined_sents')))\n",
    "    .withColumn('overall_polarity_score', get_sentiment_score(F.col('review_content')))\n",
    ")\n",
    "\n",
    "clustered_asp_pdf = scored_asp_sdf.drop_duplicates(subset=['normalized_url', 'review_source_id']).toPandas()\n",
    "\n",
    "overall_pdf = scored_asp_sdf[['normalized_url', 'review_source_id', 'overall_polarity_score']].drop_duplicates().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.337143Z",
     "start_time": "2022-02-02T15:38:40.845Z"
    }
   },
   "outputs": [],
   "source": [
    "pivot_asp = (\n",
    "    clustered_asp_pdf\n",
    "    .set_index(['normalized_url', 'review_source_id', 'top_asp'])[['polarity_score']]\n",
    "    .unstack('top_asp')\n",
    ")\n",
    "pivot_asp.columns = pivot_asp.columns.get_level_values(1)\n",
    "pivot_asp = pivot_asp.reset_index()\n",
    "pivot_asp = pivot_asp.merge(overall_pdf, on=['normalized_url', 'review_source_id'], how='left')\n",
    "\n",
    "for asp in aspect_li + ['overall_polarity_score']:\n",
    "    pivot_asp[asp] = 3 + 2 * pivot_asp[asp] # map to [1,5] stars from [-1, 1] scale\n",
    "\n",
    "avg_sentiment_pdf = pivot_asp.groupby('normalized_url').describe().transpose()\n",
    "avg_sentiment_pdf = avg_sentiment_pdf[avg_sentiment_pdf.index != 'overall_polarity_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.337770Z",
     "start_time": "2022-02-02T15:38:40.854Z"
    }
   },
   "outputs": [],
   "source": [
    "pivot_asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.338452Z",
     "start_time": "2022-02-02T15:38:40.857Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_pivot_asp = pivot_asp.copy(True)\n",
    "for asp in aspect_li:\n",
    "    imp_pivot_asp[asp] = imp_pivot_asp[asp].fillna(pivot_asp.overall_polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.339077Z",
     "start_time": "2022-02-02T15:38:40.860Z"
    }
   },
   "outputs": [],
   "source": [
    "# truth_pdf = brand_revs.select(aspect_li + ['review_rating', 'review_source_id']).toPandas()\n",
    "truth_pdf = brand_revs.select(['normalized_url', 'review_rating', 'review_source_id']).toPandas()\n",
    "\n",
    "feature_pdf = (\n",
    "    pivot_asp\n",
    "    .merge(truth_pdf, how='left', on=['normalized_url', 'review_source_id'], suffixes=('', '_true'))\n",
    "    .set_index(['normalized_url', 'review_source_id'])\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "imp_feature_pdf = (\n",
    "    imp_pivot_asp\n",
    "    .merge(truth_pdf, how='left', on=['normalized_url', 'review_source_id'], suffixes=('', '_true'))\n",
    "    .set_index(['normalized_url', 'review_source_id'])\n",
    "    .astype(float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.339676Z",
     "start_time": "2022-02-02T15:38:40.869Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_feature_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.340264Z",
     "start_time": "2022-02-02T15:38:40.872Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking correlations between rating and polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.340882Z",
     "start_time": "2022-02-02T15:38:40.875Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_feature_pdf.groupby('normalized_url').corr('pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regression is fitting a model of the aspect weights to the review rating; seeing which matter most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.341498Z",
     "start_time": "2022-02-02T15:38:40.880Z"
    }
   },
   "outputs": [],
   "source": [
    "X = imp_feature_pdf[aspect_li].values\n",
    "Y = imp_feature_pdf[['review_rating']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.342185Z",
     "start_time": "2022-02-02T15:38:40.883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "est = sm.OLS(Y, X)\n",
    "results = est.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.342797Z",
     "start_time": "2022-02-02T15:38:40.886Z"
    }
   },
   "outputs": [],
   "source": [
    "nonneg_wts = [w if w > 0 else 0 for w in results.params]\n",
    "normalized_sum = __builtins__.sum(nonneg_wts)\n",
    "normalized_weights = nonneg_wts / normalized_sum\n",
    "normalized_bse = results.bse / normalized_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.343559Z",
     "start_time": "2022-02-02T15:38:40.889Z"
    }
   },
   "outputs": [],
   "source": [
    "p_vals_pdf = pd.DataFrame(results.pvalues)\n",
    "p_vals_pdf.index = aspect_li\n",
    "p_vals_pdf.columns = ['p_value']\n",
    "p_vals_pdf['significant_at_0.05'] = p_vals_pdf['p_value'] <= 0.05\n",
    "p_vals_pdf = p_vals_pdf.round(decimals=3)\n",
    "p_vals_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.344218Z",
     "start_time": "2022-02-02T15:38:40.896Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.344844Z",
     "start_time": "2022-02-02T15:38:40.899Z"
    }
   },
   "outputs": [],
   "source": [
    "mape_overall = mean_absolute_percentage_error(Y, results.fittedvalues)\n",
    "print(f'The mean absolute percentage error is {round(mape_overall,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then taking at the brand level, which aspects predict the overall rating most? For one brand, Texture may drive overall rating, while for others, it could be Scent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.345464Z",
     "start_time": "2022-02-02T15:38:40.901Z"
    }
   },
   "outputs": [],
   "source": [
    "mapes = []\n",
    "df_holder = []\n",
    "for url, data in imp_feature_pdf.groupby('normalized_url'):\n",
    "    X = data[aspect_li].values\n",
    "    Y = data[['review_rating']].values\n",
    "    est = sm.OLS(Y, X)\n",
    "    results = est.fit()\n",
    "    \n",
    "    pos_wts = [w if w > 0 else 0 for w in results.params]\n",
    "    \n",
    "    norm_sum = __builtins__.sum(pos_wts)\n",
    "    norm_weights = pos_wts / norm_sum\n",
    "    norm_bse = results.bse / norm_sum\n",
    "    \n",
    "    data = {'weights': norm_weights, 'std': norm_bse, 'p_value': results.pvalues}\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df.columns = columns=aspect_li\n",
    "    df = df.transpose().rename_axis('aspect').reset_index()\n",
    "    df['normalized_url'] = url\n",
    "    df['brand_name'] = df.normalized_url.map(brand_name_di)\n",
    "    df_holder.append(df)\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(Y, results.fittedvalues)\n",
    "    mapes.append((brand_name_di[url], mape))\n",
    "\n",
    "brand_wt_pdf = pd.concat(df_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.346251Z",
     "start_time": "2022-02-02T15:38:40.903Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_wt_pdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.346938Z",
     "start_time": "2022-02-02T15:38:40.905Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_stats = brand_wt_pdf[['aspect', 'p_value', 'brand_name']]\n",
    "brand_stats = brand_stats.round(decimals=3)\n",
    "brand_stats['significant_at_0.05'] = brand_stats['p_value'] <= 0.05\n",
    "col_order = ['brand_name', 'aspect', 'p_value', 'significant_at_0.05']\n",
    "brand_stats[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.347535Z",
     "start_time": "2022-02-02T15:38:40.907Z"
    }
   },
   "outputs": [],
   "source": [
    "mapes_pdf = pd.DataFrame(mapes)\n",
    "mapes_pdf.columns = ['brand_name', 'mape']\n",
    "mapes_pdf.round(decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, making a data frame with each aspect, each brand name, and its median rating as well as relative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.348202Z",
     "start_time": "2022-02-02T15:38:40.910Z"
    }
   },
   "outputs": [],
   "source": [
    "rating_data = avg_sentiment_pdf.stack().to_frame().reset_index()\n",
    "rating_data.columns=['aspect', 'stat', 'normalized_url', 'median']\n",
    "rating_data = rating_data[(rating_data['stat']=='50%') & (rating_data['aspect']!='overall_polarity_score')].drop('stat', axis=1)\n",
    "rating_data['brand_name'] = rating_data['normalized_url'].map(brand_name_di)\n",
    "rating_data['weights'] = rating_data['aspect'].map(dict(zip(aspect_li, normalized_weights)))\n",
    "rating_data = rating_data.sort_values(['normalized_url', 'weights'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.348984Z",
     "start_time": "2022-02-02T15:38:40.912Z"
    }
   },
   "outputs": [],
   "source": [
    "rating_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T19:15:08.695108Z",
     "start_time": "2021-08-24T19:15:08.692714Z"
    }
   },
   "source": [
    "## Plot Ratings, without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.349567Z",
     "start_time": "2022-02-02T15:38:40.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Was having issues with Plotly producing blank outputs, so I added this (from: https://stackoverflow.com/questions/48560138/plotly-offline-iplot-gives-a-large-blank-field-as-its-output-in-jupyter-notebook)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.350183Z",
     "start_time": "2022-02-02T15:38:40.916Z"
    }
   },
   "outputs": [],
   "source": [
    "# review count\n",
    "review_count_dict = imp_feature_pdf.reset_index().groupby('normalized_url')['review_source_id'].nunique().to_dict()\n",
    "labels_w_counts = {\n",
    "    brand_name_di[k]: f'{brand_name_di[k]} ({str(v)} reviews)'\n",
    "    for k, v in review_count_dict.items()\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.350801Z",
     "start_time": "2022-02-02T15:38:40.918Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    rating_data,\n",
    "    x='aspect',\n",
    "    y=\"median\",\n",
    "    color='brand_name',\n",
    "    text='median',\n",
    "#     error_y=\"std\",\n",
    "    labels = {'brand_name': 'Brand'},\n",
    "    color_discrete_sequence=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "    barmode='group',\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textposition='outside',\n",
    "    texttemplate='%{text:.2s}',\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Aspect Ratings\",\n",
    "    xaxis_title=\"Aspect\",\n",
    "    yaxis_title=\"Star Ratings (Out of 5)\",\n",
    "    yaxis_range=[1,5]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    font=Font.plot_title.value,\n",
    "    plot_bgcolor=\"white\",\n",
    "    title={\"x\": 0.5},\n",
    "    colorway=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "     colorscale={\"sequential\": CU_PLOTLY_COLORSCALE},\n",
    "    coloraxis={\"autocolorscale\": True},\n",
    ")\n",
    "\n",
    "fig.for_each_trace(lambda t: t.update(name = labels_w_counts[t.name]))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rating along with weights\n",
    "\n",
    "This is too busy, will remove standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.351469Z",
     "start_time": "2022-02-02T15:38:40.920Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "line_data = px.scatter(\n",
    "        rating_data,\n",
    "        x='aspect',\n",
    "        y=\"median\",\n",
    "        color='brand_name',\n",
    "        color_discrete_sequence=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "        category_orders={\"brand_name\": BRAND_NAME_LI}\n",
    "    )\n",
    "\n",
    "bar_data =  px.bar(\n",
    "        brand_wt_pdf,\n",
    "        x='aspect',\n",
    "        y='weights',\n",
    "        error_y='std',\n",
    "        color='brand_name',\n",
    "        barmode='group',\n",
    "        color_discrete_sequence=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "        category_orders={\"brand_name\": BRAND_NAME_LI}\n",
    "    )\n",
    "\n",
    "for data in line_data.data:\n",
    "    fig.add_trace(data.update(mode='markers+lines'), secondary_y=True)\n",
    "    \n",
    "for data in bar_data.data:\n",
    "    fig.add_trace(data)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"KPC for Select {GROUP_NAME} Brands\",\n",
    "    xaxis_title=\"Aspect\",\n",
    "    yaxis_title=\"Overall Normalized Percent Weight\",\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "#     categoryorder='array',\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    range=[1,5],\n",
    "    title=\"Brand Stars Rating (out of 5)\",\n",
    "    secondary_y=True\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    range=[0,1],\n",
    "    tickformat='.0%',\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    font=Font.plot_title.value,\n",
    "    plot_bgcolor=\"white\",\n",
    "    title={\"x\": 0.5},\n",
    "    colorway=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "    colorscale={\"sequential\": CU_PLOTLY_COLORSCALE},\n",
    "    coloraxis={\"autocolorscale\": True},\n",
    ")\n",
    "\n",
    "fig.for_each_trace(lambda t: t.update(name = labels_w_counts[t.name]) if t.type =='scatter' else None)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.352131Z",
     "start_time": "2022-02-02T15:38:40.922Z"
    }
   },
   "outputs": [],
   "source": [
    "min = __builtins__.min\n",
    "max = __builtins__.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.352775Z",
     "start_time": "2022-02-02T15:38:40.928Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "line_data = px.scatter(\n",
    "        rating_data,\n",
    "        x='aspect',\n",
    "        y=\"median\",\n",
    "        color='brand_name',\n",
    "        color_discrete_sequence=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "        category_orders={\"brand_name\": BRAND_NAME_LI}\n",
    "    )\n",
    "\n",
    "for data in line_data.data:\n",
    "    data['line']['width'] = 5\n",
    "    fig.add_trace(data.update(mode='lines+markers'), secondary_y=True)\n",
    "    fig.update_traces(marker=dict(size=12,line=dict(width=2,color='DarkSlateGrey')))\n",
    "\n",
    "fig.add_trace(\n",
    "    px.bar(\n",
    "        x=aspect_li,\n",
    "        y=normalized_weights,\n",
    "        #error_y=normalized_bse,\n",
    "        color_discrete_sequence=[Color.grey_2.value],\n",
    "        barmode='group',\n",
    "        opacity=0.5,\n",
    "    ).data[0],\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Key Purchase Criteria for Select {GROUP_NAME} Brands\",\n",
    "    xaxis_title=\"Aspect\",\n",
    "    yaxis_title=\"Importance (Percent Weight)\",\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "#     categoryorder='total descending'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    range=[min(rating_data['median'])-0.1, max(rating_data['median'])+0.1],\n",
    "    title=\"Brand Stars Rating (out of 5)\",\n",
    "    secondary_y=True\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    range=[0, max(normalized_weights)+max(normalized_bse)],\n",
    "    tickformat='.0%',\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    font=Font.plot_title.value,\n",
    "    plot_bgcolor=\"white\",\n",
    "    title={\"x\": 0.5},\n",
    "    colorway=CU_PLOTLY_COLOR_SEQUENCE,\n",
    "    colorscale={\"sequential\": CU_PLOTLY_COLORSCALE},\n",
    "    coloraxis={\"autocolorscale\": True},\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        titlefont=dict(\n",
    "            color=Color.grey_2.value\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color=Color.grey_2.value\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.for_each_trace(lambda t: t.update(name = labels_w_counts[t.name]) if t.name else None)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA - Review after script has run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.353604Z",
     "start_time": "2022-02-02T15:38:40.930Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_name_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.354316Z",
     "start_time": "2022-02-02T15:38:40.933Z"
    }
   },
   "outputs": [],
   "source": [
    "product_name_include_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.354958Z",
     "start_time": "2022-02-02T15:38:40.935Z"
    }
   },
   "outputs": [],
   "source": [
    "product_name_exclude_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.355585Z",
     "start_time": "2022-02-02T15:38:40.937Z"
    }
   },
   "outputs": [],
   "source": [
    "def qa_dictionaries(brand_name_di, product_name_include_di, product_name_exclude_di):\n",
    "    if len(brand_name_di.items()) > len(product_name_include_di.items()):\n",
    "        print('!!! FLAG - you may be excluding brands accidently. Must add more entries to product dictionaries.')\n",
    "    else:\n",
    "        print('Product name include dictionary length is good')\n",
    "    if len(brand_name_di.items()) > len(product_name_exclude_di.items()):\n",
    "        print('!!! FLAG - you may be excluding brands accidently. Must add more entries to product dictionaries.')\n",
    "    else:\n",
    "        print('Product name exclude dictionary length is good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.356214Z",
     "start_time": "2022-02-02T15:38:40.939Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_dictionaries(brand_name_di, product_name_include_di, product_name_exclude_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.356825Z",
     "start_time": "2022-02-02T15:38:40.941Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Test QA Function\n",
    "# brand_name_di_test = brand_name_di\n",
    "# brand_name_di_test['test_item'] = 'test'\n",
    "# brand_name_di_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.357425Z",
     "start_time": "2022-02-02T15:38:40.943Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Test\n",
    "# qa_dictionaries(brand_name_di_test, product_name_include_di, product_name_exclude_di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View DF Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.358037Z",
     "start_time": "2022-02-02T15:38:40.945Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.358899Z",
     "start_time": "2022-02-02T15:38:40.947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helpers function will create list of all dataframes in notebook in chronological order (from when they are run in the script)\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def list_all_dataframes():\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, pd.DataFrame) or isinstance(v, DataFrame)]\n",
    "list_all_df = list_all_dataframes()\n",
    "len(list_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:39:15.359681Z",
     "start_time": "2022-02-02T15:38:40.957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter defined at top of script\n",
    "if print_all_dataframes == 'Y':\n",
    "\n",
    "    # Write a function that will print both Pandas and Spark dataframes\n",
    "    for df in list_all_df:\n",
    "        df_name = df\n",
    "        df = df.replace(\"'\", \"\")\n",
    "        df = locals()[df]\n",
    "        print('df - ' + df_name )\n",
    "        print(type(df))\n",
    "        try: \n",
    "            display(df.limit(5).toPandas())\n",
    "        except Exception:\n",
    "            try:\n",
    "                display(df.head(5))\n",
    "            except:\n",
    "                print('ERROR - ' + df_name)            \n",
    "\n",
    "else:\n",
    "    print('parameter indicates print_all_dataframes is not requested, change to Y at top of script if interested')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "282px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "800.938px",
    "left": "0px",
    "right": "1488px",
    "top": "111.051px",
    "width": "191.989px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
