{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "23c25172-9087-4bb9-8f60-b9b4148a764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import contractions\n",
    "import math\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import string\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a2b7688-26d0-469e-946a-039857a5256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"flexfits_emails_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd209b4a-214d-4a98-b5d0-cfbed66a5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1= pd.read_csv(\"flexfits_emails_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033499fb-3288-4c3d-b506-534a3265aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "58fe94c1-9318-4196-96af-2de11b55b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_CUSTOM_STOPWORDS = [\n",
    "    \"bike\", \"nbsp\", \"crn\",\"sample\", \"bikes\", \"great\", \"good\", \"love\",\"like\", \"recommend\",\"smells\",\"collected\",\"part\",\"review\",\"promotion\",\"razor\",\n",
    "    \"shave\",\"shaving\",\"blade\",\"camille\",\"cantu\",\"daughter\",\"today\",\"kccc\",\"jessie\",\"miss\",\"fabric\",\"pillow\",\"softener\",\"or\",    \"tiktok\",\"OR\",\n",
    "     'Proov', 'proov','Natalist', 'natalist','Stix', 'stix','Clear Blue', 'blue','Modern Fertility', 'modern', 'free', 'app', 'mf', 'android', 'pinchme',\n",
    "    'Pregmate', 'pregmate', 'received','First Response', 'test', 'ovulation', 'none', 'see', 'store', 'day', 'month', 'amy', 'face', 'far', 'scan',\n",
    "    'it‚äôs','don‚äôt', 'using_minoxidil', '_oz','i‚äôm','moreread','using','stating',\n",
    "    'doesn‚äôt', 'classic','order','ordering', 'best', 'hard', 'seltzer', 'claw','never','got','ordered','white','product','high-noon','bottle','girl',\n",
    "    'read_lessread','read_moreread','lessread_le','le_stating','moreread_stating','scalp detox','le_serum',\n",
    "    'moreread_serum','i’ve_stem','hair_read','don’t_know','i’ve_bought', 'liquid_kinda', \"i'ḿ\"'use_serum','le_mus','read_serum','ingredient_us','oz_extreme',\n",
    "    'cell_serum','cell','stem','le','read','lessread','moreread','more','multi-colored—from_brown','findin_read','strengthening_hair','hair_oil','care_oil',\n",
    "    'hair','oil_hair','extreme_hair','hair_care','extreme','oz','_strengthening','used twice', 'oil_oil', 'customer',    'nan', 'read_moreread', 'noreferrer',\n",
    "    'noreferrer_farmer','farmer_dog/a','noreferrer_ollie/a','rel','noopener','blank_rel',\n",
    "    'rel_nofollow','blank','nofollow','target_blank','noopener_noreferrer','nofollower_noopener','nofollow_noopener',\"partake\",\"partake_cooky\", \"crunchy_cooky\",\"cooky_vegan\",\"madegood\",\"apple_jack\"\n",
    "]\n",
    "\n",
    "added_stopwords_li = [\n",
    "    \"it’s\",    \"'d\",    \"'s\",\n",
    "    \"n't\",    \"'m\",\n",
    "    \"i've\",    \"it's\",    \"'ve\",\n",
    "    \"'re\",    \"'ll\",    \"``\",    \"''\",    \"...\",    \"--\", \"https\",\n",
    "    \"voxbox\",    \"influenster\",    \"cracker\",    \"or\",    'it‚äôs',\n",
    "    'don‚äôt',    'i‚äôm',    'doesn‚äôt',    \"get\", \"also\",    \"even\",    \"since\", \"amy\"\n",
    "]\n",
    "\n",
    "REV_COLS = ['source_name',\n",
    "    'normalized_url',\n",
    "    'review_source_id',\n",
    "    'reviewer_source_id',\n",
    "    'brand_name',\n",
    "    'product_name',\n",
    "    'product_source_id',\n",
    "    'review_date',\n",
    "    'review_rating',\n",
    "    'review_content'\n",
    "]\n",
    "\n",
    "product_categories = ['Accessories',\n",
    "'Alcoholic Beverages',\n",
    "'Apparel & Footwear',\n",
    "'Food',\n",
    "'Food Establishments',\n",
    "'Health & Beauty Establishments',\n",
    "'Household Consumables',\n",
    "'Household Durables',\n",
    "'Other',\n",
    "'Pet Products',\n",
    "'Non-Alcoholic Beverages',\n",
    "'Personal Care & Beauty',\n",
    "'Retailers',\n",
    "'Sports & Outdoors'\n",
    "]\n",
    "\n",
    "drop_words = [\"kinship\",\n",
    "              \"mineral\",\n",
    "              \"trendmood\",\n",
    "              \"hue_stick\",\n",
    "              \"huesticks\",\n",
    "              \"stick\",\n",
    "              \"blume\",\n",
    "              \"daydreamer\",\n",
    "              \"loli\",\n",
    "              \"plum\",\n",
    "              \"elixir\",\n",
    "              \"nut\",\n",
    "              \"beekman\",\n",
    "              \"goat\",\n",
    "              \"milk\",\n",
    "              \"bloom\",\n",
    "              \"fifth_root\",\n",
    "              \"fifth\",\n",
    "              \"lip\",\n",
    "              \"cannuka\",\n",
    "              \"review\",\n",
    "              \"zitsticka\",\n",
    "              \"discipline\",\n",
    "              \"uoma\",\n",
    "              \"bv\",\n",
    "              \"yeast\",\n",
    "              \"bye_bloat\",\n",
    "              \"tula\",\n",
    "              \"tula_product\",\n",
    "              \"huestick\",\n",
    "              \"hue\",\n",
    "              \"live_tinted\",\n",
    "              \"meltdown\",\n",
    "              \"loli_product\",\n",
    "              \"bloom_cream\",\n",
    "              \"beekman_product\",\n",
    "              \"pure_goat\",\n",
    "              \"karmic_cleanse\",\n",
    "              \n",
    "             ]\n",
    "\n",
    "MIN_TERM_PCT = 0\n",
    "\n",
    "MAX_TERM_PCT = 100\n",
    "\n",
    "TERM_PERCENT_HEAD = 1\n",
    "\n",
    "TOP_N_WORDS = 8\n",
    "\n",
    "np.random.seed(42) # set seed for models for reproducibility\n",
    "\n",
    "stopwords_li = stopwords.words('english')\n",
    "punkts_li = list(string.punctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7de1e703-fcec-48a3-8a61-7c1e82b5ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s, strip_str=\"=-_/\\+.:,'* 1234567890—\"):\n",
    "    custom_words_li = GROUP_CUSTOM_STOPWORDS + added_stopwords_li+stopwords_li + punkts_li + [\"\"]\n",
    "    return [w.lower().strip(strip_str) for w in word_tokenize(str(s)) if w.lower().strip(strip_str) not in custom_words_li]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "def unique_list(li):\n",
    "    deduped_li = list(set(li))\n",
    "    return deduped_li\n",
    "\n",
    "def bigram(tokens):\n",
    "    bi_tup = list(bigrams(tokens))\n",
    "    bi_li = ['_'.join(tup) for tup in bi_tup]\n",
    "    return bi_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ea72ea12-1d84-46b7-95c1-86a8c23475de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove repeat emails\n",
    "\n",
    "df = df[\"Description\"].drop_duplicates()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d50f6-0fdb-4997-a2a6-8e23d533c472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "147988f5-e503-485c-8a99-6c1af4533374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n?\\nHi,\\nI was hoping to speak with you about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n------------------\\nSubmitted from: https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n------------------\\nSubmitted from: https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\"Mothering our Mothers\" workshop Spring 2019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>( I wrote you before, but failed to notice I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>3734</td>\n",
       "      <td>Can I please cancel monthly shipping order? I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>3735</td>\n",
       "      <td>Can I please cancel my account? I am no longer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>3736</td>\n",
       "      <td>Can I please cancel my account? I no longer ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>3737</td>\n",
       "      <td>Can I please cancel my account. \\nThank you!\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>3738</td>\n",
       "      <td>Can I please cancel my auto ship orders?  I re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3739 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                        Description\n",
       "0         0  \\n?\\nHi,\\nI was hoping to speak with you about...\n",
       "1         1   \\n\\n------------------\\nSubmitted from: https...\n",
       "2         2   \\n\\n------------------\\nSubmitted from: https...\n",
       "3         3  \\n\"Mothering our Mothers\" workshop Spring 2019...\n",
       "4         4   ( I wrote you before, but failed to notice I ...\n",
       "...     ...                                                ...\n",
       "3734   3734  Can I please cancel monthly shipping order? I ...\n",
       "3735   3735  Can I please cancel my account? I am no longer...\n",
       "3736   3736  Can I please cancel my account? I no longer ne...\n",
       "3737   3737  Can I please cancel my account. \\nThank you!\\n...\n",
       "3738   3738  Can I please cancel my auto ship orders?  I re...\n",
       "\n",
       "[3739 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "39d70756-5193-4828-9953-0a87c1551005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_email\"] = df[\"Description\"].apply(lambda x: lemmatize(tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2d4f3c9d-c035-47e5-b14b-58161691f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Description</th>\n",
       "      <th>tokenized_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n?\\nHi,\\nI was hoping to speak with you about...</td>\n",
       "      <td>[hi, hoping, speak, ultra-luxury, community, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n------------------\\nSubmitted from: https...</td>\n",
       "      <td>[submitted, flexfits.com/account]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n------------------\\nSubmitted from: https...</td>\n",
       "      <td>[submitted, flexfits.com/blogs/thefixx/10-tips...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\"Mothering our Mothers\" workshop Spring 2019...</td>\n",
       "      <td>[mothering, mother, workshop, spring, atlanta,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>( I wrote you before, but failed to notice I ...</td>\n",
       "      <td>[wrote, failed, notice, wrote, wrong, email, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>3734</td>\n",
       "      <td>Can I please cancel monthly shipping order? I ...</td>\n",
       "      <td>[please, cancel, monthly, shipping, many, righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>3735</td>\n",
       "      <td>Can I please cancel my account? I am no longer...</td>\n",
       "      <td>[please, cancel, account, longer, need, thank,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>3736</td>\n",
       "      <td>Can I please cancel my account? I no longer ne...</td>\n",
       "      <td>[please, cancel, account, longer, need, flex, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>3737</td>\n",
       "      <td>Can I please cancel my account. \\nThank you!\\n...</td>\n",
       "      <td>[please, cancel, account, thank, beleive, paid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>3738</td>\n",
       "      <td>Can I please cancel my auto ship orders?  I re...</td>\n",
       "      <td>[please, cancel, auto, ship, order, really, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3739 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                        Description  \\\n",
       "0         0  \\n?\\nHi,\\nI was hoping to speak with you about...   \n",
       "1         1   \\n\\n------------------\\nSubmitted from: https...   \n",
       "2         2   \\n\\n------------------\\nSubmitted from: https...   \n",
       "3         3  \\n\"Mothering our Mothers\" workshop Spring 2019...   \n",
       "4         4   ( I wrote you before, but failed to notice I ...   \n",
       "...     ...                                                ...   \n",
       "3734   3734  Can I please cancel monthly shipping order? I ...   \n",
       "3735   3735  Can I please cancel my account? I am no longer...   \n",
       "3736   3736  Can I please cancel my account? I no longer ne...   \n",
       "3737   3737  Can I please cancel my account. \\nThank you!\\n...   \n",
       "3738   3738  Can I please cancel my auto ship orders?  I re...   \n",
       "\n",
       "                                        tokenized_email  \n",
       "0     [hi, hoping, speak, ultra-luxury, community, f...  \n",
       "1                     [submitted, flexfits.com/account]  \n",
       "2     [submitted, flexfits.com/blogs/thefixx/10-tips...  \n",
       "3     [mothering, mother, workshop, spring, atlanta,...  \n",
       "4     [wrote, failed, notice, wrote, wrong, email, w...  \n",
       "...                                                 ...  \n",
       "3734  [please, cancel, monthly, shipping, many, righ...  \n",
       "3735  [please, cancel, account, longer, need, thank,...  \n",
       "3736  [please, cancel, account, longer, need, flex, ...  \n",
       "3737  [please, cancel, account, thank, beleive, paid...  \n",
       "3738  [please, cancel, auto, ship, order, really, th...  \n",
       "\n",
       "[3739 rows x 3 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78cc835b-9628-4cfb-9dfb-04eba41f5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['tokenized_email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5b07a136-80ad-40e5-8f8d-de1abe691f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unibi'] = df['tokenized_email'].apply(lambda x: unique_list(bigram(x)+x))\n",
    "df = df.loc[df.astype(str).drop_duplicates().index]\n",
    "df['unibi_word'] = df['unibi']\n",
    "df = df.explode('unibi_word')\n",
    "df= df[~df[\"unibi_word\"].isin(drop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "59ab2f39-6c1c-4036-aec3-e744479bdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"unibi_word\"].value_counts().reset_index(name=\"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2d337b3e-5467-4b05-8fa8-abdfe06d9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter words\n",
    "filter_words = [\"flex\",\n",
    "               \"hello\",\n",
    "               \".com\",\n",
    "                \"email\",\n",
    "                \"sent\",\n",
    "                \"wrote\",\n",
    "                \"hi\",\n",
    "                \"iphone\",\n",
    "                \"please\",\n",
    "                \"company\",\n",
    "                \"thank\",\n",
    "                \"you\",\n",
    "                \":\",\n",
    "                \",\"\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2dd8f8b7-7e28-47dc-84a4-d5b58a2a72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts[ ~counts[\"index\"].str.contains(\"|\".join(filter_words))]\n",
    "counts = counts[counts[\"counts\"]>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fcbf63ca-611c-4f2b-9988-48e89d608da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_di = dict(zip(counts[\"index\"],counts[\"counts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "30905c1d-2a11-4f81-8f6d-2ac541f135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number of times word appeard\"]=df[\"unibi_word\"].map(counts_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "95c56bb5-0706-4486-99ea-09e9c3779e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fdec9072-88bc-41cd-8a42-30af2ce29d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"flexfit_email_word_frequency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2e0b0c10-3aca-4807-a1ac-4590fa68238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.to_csv(\"flexfit_word_frequency_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce4ea2-5565-48e0-8c90-6bb6b7ba9348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
